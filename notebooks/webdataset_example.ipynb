{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "empty-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import webdataset as wds\n",
    "import json\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "promising-chemistry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"../data/\"\n",
    "\n",
    "# Opening JSON file\n",
    "with open('../parameters.json') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "active-evening",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 512, 512)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "studynames = pickle.load(open(os.path.join(data_dir,'CTscans_studynames_train_100'), 'rb'))\n",
    "labels = pickle.load(open(os.path.join(data_dir,'CTscans_3Dlabels_train_100'), 'rb'))\n",
    "volumes = pickle.load(open(os.path.join(data_dir,'CTscans_3Dvolumes_train_100'), 'rb'))\n",
    "volumes[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "apart-writing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 6)\n",
      "(100,)\n",
      "(100, 12, 512, 512)\n"
     ]
    }
   ],
   "source": [
    "labels_np =np.array(labels)\n",
    "studynames_np = np.array(studynames)\n",
    "volumes_np = np.array(volumes)\n",
    "print(labels_np.shape)\n",
    "print(studynames_np.shape)\n",
    "print(volumes_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aware-edmonton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([studynames_np[idx]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "australian-associate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# writing ../data/shards/shard-000000.tar 0 0.0 GB 0\n",
      "# writing ../data/shards/shard-000001.tar 32 0.8 GB 32\n",
      "# writing ../data/shards/shard-000002.tar 32 0.8 GB 64\n",
      "# writing ../data/shards/shard-000003.tar 32 0.8 GB 96\n"
     ]
    }
   ],
   "source": [
    "# Code credits: From webdataset GitHub repo getting started\n",
    "with wds.ShardWriter(os.path.join(data_dir, 'shards', 'shard-%06d.tar'), maxcount=parameters['batch_size']) as sink:\n",
    "    for idx in range(len(studynames_np)):\n",
    "        sink.write({\n",
    "            '__key__': \"%06d\"%idx,\n",
    "            'volumes.pyd': volumes_np[idx, :, :, :],\n",
    "            'labels.pyd': labels_np[idx, :],\n",
    "            'studynames.pyd': np.array([studynames_np[idx]])\n",
    "            })\n",
    "\n",
    "# shard_idx = 0\n",
    "# for df_idx in np.arange(0, len(studynames_np), parameters['batch_size']):\n",
    "#     dirname = os.path.join(data_dir, 'shards', 'shard-%06d' % shard_idx)\n",
    "    \n",
    "#     # Make directory\n",
    "#     os.system(\"mkdir \" + dirname)\n",
    "    \n",
    "#     # Save numpy arrays\n",
    "#     start = df_idx\n",
    "#     end = min(start + parameters['batch_size'], len(studynames_np))\n",
    "    \n",
    "#     np.save(os.path.join(dirname, 'volumes.npy'), volumes_np[start:end, :, :, :])\n",
    "#     np.save(os.path.join(dirname, 'labels.npy'), labels_np[start:end, :])\n",
    "#     np.save(os.path.join(dirname, 'studynames.npy'), studynames_np[start:end])\n",
    "    \n",
    "#     # Then tar the directory\n",
    "#     os.system('tar cvf ' + dirname +'.tar ' + dirname)\n",
    "    \n",
    "#     # And remove the directory\n",
    "#     os.system('rm -rf ' + dirname)\n",
    "    \n",
    "#     shard_idx += 1\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "# for df_idx in np.arange(0, len(studynames_np), parameters['batch_size']):\n",
    "#     dirname = os.path.join(data_dir, 'shards', 'shard-%06d' % shard_idx)\n",
    "    \n",
    "#     with open(fname, 'wb') as shardfile:\n",
    "#         start = df_idx\n",
    "#         end = min(start + parameters['batch_size'], len(studynames_np))\n",
    "#         save_dat = {'__key__': \"%06d\" % shard_idx, 'volumes': volumes_np[start:end, :, :, :], \n",
    "#                     'labels': labels_np[start:end, :], 'studynames': studynames_np[start:end]}\n",
    "#         np.savez(shardfile, **save_dat)\n",
    "#         shardfile.close()\n",
    "#     print(\"Finished writing file:\", fname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "utility-vitamin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-r--r--r-- bigdata/bigdata 201 2021-05-11 06:38 000000.labels.pyd\n",
      "-r--r--r-- bigdata/bigdata 25165986 2021-05-11 06:38 000000.volumes.pyd\n",
      "-r--r--r-- bigdata/bigdata      201 2021-05-11 06:38 000001.labels.pyd\n",
      "-r--r--r-- bigdata/bigdata 25165986 2021-05-11 06:38 000001.volumes.pyd\n",
      "-r--r--r-- bigdata/bigdata      201 2021-05-11 06:38 000002.labels.pyd\n",
      "-r--r--r-- bigdata/bigdata 25165986 2021-05-11 06:38 000002.volumes.pyd\n",
      "-r--r--r-- bigdata/bigdata      201 2021-05-11 06:38 000003.labels.pyd\n",
      "-r--r--r-- bigdata/bigdata 25165986 2021-05-11 06:38 000003.volumes.pyd\n",
      "-r--r--r-- bigdata/bigdata      201 2021-05-11 06:38 000004.labels.pyd\n",
      "-r--r--r-- bigdata/bigdata 25165986 2021-05-11 06:38 000004.volumes.pyd\n",
      "tar: write error\n"
     ]
    }
   ],
   "source": [
    "!tar tvf ../data/shards/shard-000000.tar | head\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m65",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m65"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
