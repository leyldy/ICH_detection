{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Code credits: Adapted bits and pieces from https://github.com/webdataset/webdataset/blob/master/docs/gettingstarted.ipynb\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from itertools import islice\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform as st\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import webdataset as wds\n",
    "\n",
    "from model.selfattn_3d_cnn import *\n",
    "from model.baseline_3d_cnn import *\n",
    "from utils.model_utils import *\n",
    "from utils.model_run import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 16, 'shard_size': 16}"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../data'\n",
    "shards_dir = os.path.join(data_dir, 'shards_new')\n",
    "\n",
    "# Opening JSON file\n",
    "with open('../parameters.json') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "batch_size = 4 #parameters['batch_size']\n",
    "shard_size = 4 #parameters['shard_size']\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train shards: 2\n",
      "Number of validation shards: 1\n",
      "Number of test shards: 93\n"
     ]
    }
   ],
   "source": [
    "urls = [os.path.join(shards_dir, it) for it in os.listdir(shards_dir) if it.endswith('.tar')]\n",
    "\n",
    "# Try to overfit on smaller data\n",
    "# urls = urls[:round(len(urls)*0.3)]\n",
    "\n",
    "# Another shard directory, continued; realize can't use because keys will collide cuz we refreshed...\n",
    "# shards_dir2 = os.path.join(data_dir, 'shards_new_cont')\n",
    "# urls += [os.path.join(shards_dir2, it) for it in os.listdir(shards_dir2) if it.endswith('.tar')]\n",
    "\n",
    "\n",
    "# All the data\n",
    "# train_urls = urls[:round(len(urls)*0.7)]\n",
    "# val_urls = urls[round(len(urls)*0.7):round(len(urls)*0.85)]\n",
    "# test_urls = urls[round(len(urls)*0.85):]\n",
    "\n",
    "# Smaller data just to run model once\n",
    "train_urls = urls[:2]\n",
    "val_urls = urls[2:3]\n",
    "test_urls = urls[3:]\n",
    "\n",
    "\n",
    "print(\"Number of train shards:\", len(train_urls))\n",
    "print(\"Number of validation shards:\", len(val_urls))\n",
    "print(\"Number of test shards:\", len(test_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations per train epoch: 2\n"
     ]
    }
   ],
   "source": [
    "# Create dataset objects\n",
    "train_iternum = len(train_urls)*shard_size//batch_size\n",
    "val_iternum = len(val_urls)*shard_size//batch_size\n",
    "test_iternum = len(test_urls)*shard_size//batch_size\n",
    "\n",
    "print(\"Number of iterations per train epoch:\", train_iternum)\n",
    "\n",
    "train_dataset = (\n",
    "    wds\n",
    "    .WebDataset(train_urls, length=train_iternum)\n",
    "    .shuffle(shard_size)\n",
    "    .decode('torch')\n",
    "    .to_tuple('volumes.pyd', 'labels.pyd', 'studynames.pyd')\n",
    "    .batched(batch_size)\n",
    "#     .map_tuple(pre_transforms, identity, identity)\n",
    ")\n",
    "loader_train = torch.utils.data.DataLoader(train_dataset, num_workers=0, batch_size=None) #setting batch_size = None disables batching\n",
    "\n",
    "val_dataset = (\n",
    "    wds\n",
    "    .WebDataset(val_urls, length=val_iternum)\n",
    "    .shuffle(shard_size)\n",
    "    .decode('torch')\n",
    "    .to_tuple('volumes.pyd', 'labels.pyd', 'studynames.pyd')\n",
    "    .batched(batch_size)\n",
    ")\n",
    "loader_val = torch.utils.data.DataLoader(val_dataset, num_workers=0, batch_size=None)\n",
    "\n",
    "test_dataset = (\n",
    "    wds\n",
    "    .WebDataset(test_urls, length=test_iternum)\n",
    "    .shuffle(shard_size)\n",
    "    .decode('torch')\n",
    "    .to_tuple('volumes.pyd', 'labels.pyd', 'studynames.pyd')\n",
    "    .batched(batch_size)\n",
    ")\n",
    "loader_test = torch.utils.data.DataLoader(test_dataset, num_workers=0, batch_size=None)\n",
    "\n",
    "# for image, target in islice(dataset, 0, 2):\n",
    "#     print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "#     dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)\n",
    "print(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make log directory and checkpoint directory (DIFFERENT DIRECTORY FROM BASELINE)\n",
    "dir_nm = datetime.now(tz=pytz.utc).astimezone(timezone('US/Pacific')).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "# dir_nm = \"first_mini_c2fc2\"\n",
    "log_dir = os.path.join('../runs_experiment', dir_nm)\n",
    "os.mkdir(log_dir)\n",
    "os.mkdir(os.path.join(log_dir, 'Checkpoints'))\n",
    "\n",
    "\n",
    "# Model, optimizer, criterion\n",
    "model = baseline_3DCNN(in_num_ch=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2/2 [00:03<00:00,  1.67s/batch, loss=0.667]/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:529: UserWarning: Length of IterableDataset <webdataset.dataset.Processor object at 0x7f63f4a1f710> was reported to be 2 (when accessing len(dataloader)), but 3 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1: : 3batch [00:04,  1.52s/batch, loss=0.882]                  /opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:529: UserWarning: Length of IterableDataset <webdataset.dataset.Processor object at 0x7f63f4a1f710> was reported to be 2 (when accessing len(dataloader)), but 4 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1: : 4batch [00:06,  1.44s/batch, loss=0.834]/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:529: UserWarning: Length of IterableDataset <webdataset.dataset.Processor object at 0x7f63f4a1f710> was reported to be 2 (when accessing len(dataloader)), but 5 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1: : 4batch [00:06,  1.44s/batch, loss=0.834]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 40, 512, 512])\n",
      "torch.Size([4, 40, 512, 512])\n",
      "torch.Size([4, 40, 512, 512])\n",
      "torch.Size([4, 40, 512, 512])\n",
      "Total iteration 5, validation loss = 0.7198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: : 5batch [00:15,  4.19s/batch, loss=0.593]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:529: UserWarning: Length of IterableDataset <webdataset.dataset.Processor object at 0x7f63f4a1f710> was reported to be 2 (when accessing len(dataloader)), but 6 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1: : 6batch [00:16,  3.25s/batch, loss=1.35] /opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:529: UserWarning: Length of IterableDataset <webdataset.dataset.Processor object at 0x7f63f4a1f710> was reported to be 2 (when accessing len(dataloader)), but 7 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1: : 7batch [00:17,  2.58s/batch, loss=0.933]/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:529: UserWarning: Length of IterableDataset <webdataset.dataset.Processor object at 0x7f63f4a1f710> was reported to be 2 (when accessing len(dataloader)), but 8 samples have been fetched. \n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1: : 8batch [00:18,  2.32s/batch, loss=0.619]\n"
     ]
    }
   ],
   "source": [
    "train_loss_dict, val_loss_dict = train(model, optimizer, criterion, loader_train, loader_val, log_dir, device=device, epochs=1, val_every=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common errors and how to fix them:\n",
    "\n",
    "# Error:\n",
    "#   RuntimeError: running_mean should contain 1 elements not 8\n",
    "# Fix: One of your batchnorm 3D parameter values is off\n",
    "\n",
    "# Error:\n",
    "#   RuntimeError: CUDA out of memory.\n",
    "# Fix: Make the model / batch size smaller \n",
    "# First try to make batch size smaller. Will require longer training time possibly but does not decrease expressivity of model.\n",
    "# If need to decrease complexity of model, \n",
    "\n",
    "# Error:\n",
    "#   RuntimeError: Given groups=1, weight of size [1, 1, 1, 1, 1], expected input[8, 4, 5, 32, 32] to have 1 channels, but got 4 channels instead\n",
    "# Fix: Wrong number of in_channels in self attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 40, 512, 512])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 40, 256, 256])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(4, 40, 512, 512)\n",
    "print(a.size())\n",
    "transforms.Resize(size=(256, 256))(a).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m68"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
