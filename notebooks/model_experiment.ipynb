{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code credits: Adapted bits and pieces from https://github.com/webdataset/webdataset/blob/master/docs/gettingstarted.ipynb\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from itertools import islice\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform as st\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import webdataset as wds\n",
    "\n",
    "from model.selfattn_3d_cnn import *\n",
    "from utils.model_utils import *\n",
    "from utils.model_run import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data'\n",
    "shards_dir = os.path.join(data_dir, 'shards_new')\n",
    "\n",
    "# Opening JSON file\n",
    "with open('../parameters.json') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "batch_size = parameters['batch_size']\n",
    "shard_size = parameters['shard_size']\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [os.path.join(shards_dir, it) for it in os.listdir(shards_dir) if it.endswith('.tar')]\n",
    "\n",
    "# Try to overfit on smaller data\n",
    "# urls = urls[:round(len(urls)*0.3)]\n",
    "\n",
    "# Another shard directory, continued; realize can't use because keys will collide cuz we refreshed...\n",
    "# shards_dir2 = os.path.join(data_dir, 'shards_new_cont')\n",
    "# urls += [os.path.join(shards_dir2, it) for it in os.listdir(shards_dir2) if it.endswith('.tar')]\n",
    "\n",
    "\n",
    "# All the data\n",
    "train_urls = urls[:round(len(urls)*0.7)]\n",
    "val_urls = urls[round(len(urls)*0.7):round(len(urls)*0.85)]\n",
    "test_urls = urls[round(len(urls)*0.85):]\n",
    "\n",
    "# Smaller data just to run model once\n",
    "# train_urls = urls[:2]\n",
    "# val_urls = urls[2:3]\n",
    "# test_urls = urls[3:]\n",
    "\n",
    "\n",
    "print(\"Number of train shards:\", len(train_urls))\n",
    "print(\"Number of validation shards:\", len(val_urls))\n",
    "print(\"Number of test shards:\", len(test_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset objects\n",
    "train_iternum = len(train_urls)*shard_size//batch_size\n",
    "val_iternum = len(val_urls)*shard_size//batch_size\n",
    "test_iternum = len(test_urls)*shard_size//batch_size\n",
    "\n",
    "print(\"Number of iterations per train epoch:\", train_iternum)\n",
    "\n",
    "train_dataset = (\n",
    "    wds\n",
    "    .WebDataset(train_urls, length=train_iternum)\n",
    "    .shuffle(shard_size)\n",
    "    .decode('torch')\n",
    "    .to_tuple('volumes.pyd', 'labels.pyd', 'studynames.pyd')\n",
    "    .batched(batch_size)\n",
    "#     .map_tuple(pre_transforms, identity, identity)\n",
    ")\n",
    "loader_train = torch.utils.data.DataLoader(train_dataset, num_workers=0, batch_size=None) #setting batch_size = None disables batching\n",
    "\n",
    "val_dataset = (\n",
    "    wds\n",
    "    .WebDataset(val_urls, length=val_iternum)\n",
    "    .shuffle(shard_size)\n",
    "    .decode('torch')\n",
    "    .to_tuple('volumes.pyd', 'labels.pyd', 'studynames.pyd')\n",
    "    .batched(batch_size)\n",
    ")\n",
    "loader_val = torch.utils.data.DataLoader(val_dataset, num_workers=0, batch_size=None)\n",
    "\n",
    "test_dataset = (\n",
    "    wds\n",
    "    .WebDataset(test_urls, length=test_iternum)\n",
    "    .shuffle(shard_size)\n",
    "    .decode('torch')\n",
    "    .to_tuple('volumes.pyd', 'labels.pyd', 'studynames.pyd')\n",
    "    .batched(batch_size)\n",
    ")\n",
    "loader_test = torch.utils.data.DataLoader(test_dataset, num_workers=0, batch_size=None)\n",
    "\n",
    "# for image, target in islice(dataset, 0, 2):\n",
    "#     print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "#     dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)\n",
    "print(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make log directory and checkpoint directory (DIFFERENT DIRECTORY FROM BASELINE)\n",
    "dir_nm = datetime.now(tz=pytz.utc).astimezone(timezone('US/Pacific')).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "# dir_nm = \"first_mini_c2fc2\"\n",
    "log_dir = os.path.join('../runs_experiment', dir_nm)\n",
    "os.mkdir(log_dir)\n",
    "os.mkdir(os.path.join(log_dir, 'Checkpoints'))\n",
    "\n",
    "\n",
    "# Model, optimizer, criterion\n",
    "model = baseline_3DCNN(in_num_ch=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_dict, val_loss_dict = train(model, optimizer, criterion, loader_train, loader_val, log_dir, device=device, epochs=10, val_every=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
