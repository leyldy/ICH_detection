{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6010c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Code credits: Adapted bits and pieces from https://github.com/webdataset/webdataset/blob/master/docs/gettingstarted.ipynb\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from itertools import islice\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.transform as st\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import webdataset as wds\n",
    "\n",
    "from model.baseline_3d_cnn import *\n",
    "from utils.model_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9e9096a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 16}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../data'\n",
    "shards_dir = os.path.join(data_dir, 'shards')\n",
    "\n",
    "# Opening JSON file\n",
    "with open('../parameters.json') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "batch_size = parameters['batch_size']\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "934db9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train shards: 29\n",
      "Number of validation shards: 10\n",
      "Number of test shards: 10\n"
     ]
    }
   ],
   "source": [
    "urls = [os.path.join(shards_dir, it) for it in os.listdir(shards_dir) if it.endswith('.tar')]\n",
    "# All the data\n",
    "train_urls = urls[:round(len(urls)*0.6)]\n",
    "val_urls = urls[round(len(urls)*0.6):round(len(urls)*0.8)]\n",
    "test_urls = urls[round(len(urls)*0.8):]\n",
    "\n",
    "# Smaller data just to run model once\n",
    "# train_urls = urls[:2]\n",
    "# val_urls = urls[2:3]\n",
    "# test_urls = urls[3:]\n",
    "\n",
    "\n",
    "print(\"Number of train shards:\", len(train_urls))\n",
    "print(\"Number of validation shards:\", len(val_urls))\n",
    "print(\"Number of test shards:\", len(test_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fbcf05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset objects\n",
    "train_dataset = (\n",
    "    wds\n",
    "    .WebDataset(train_urls)\n",
    "#     .shuffle(batch_size)\n",
    "    .decode('torch')\n",
    "    .to_tuple('volumes.pyd', 'labels.pyd', 'studynames.pyd')\n",
    "#     .map_tuple(pre_transforms, identity, identity)\n",
    ")\n",
    "loader_train = torch.utils.data.DataLoader(train_dataset.batched(batch_size), num_workers=0, batch_size=None) #setting batch_size = None disables batching\n",
    "val_dataset = (\n",
    "    wds\n",
    "    .WebDataset(val_urls)\n",
    "    .shuffle(batch_size)\n",
    "    .decode('torch')\n",
    "    .to_tuple('volumes.pyd', 'labels.pyd', 'studynames.pyd')\n",
    ")\n",
    "loader_val = torch.utils.data.DataLoader(val_dataset.batched(batch_size), num_workers=0, batch_size=None)\n",
    "\n",
    "test_dataset = (\n",
    "    wds\n",
    "    .WebDataset(test_urls)\n",
    "    .shuffle(batch_size)\n",
    "    .decode('torch')\n",
    "    .to_tuple('volumes.pyd', 'labels.pyd', 'studynames.pyd')\n",
    ")\n",
    "loader_test = torch.utils.data.DataLoader(test_dataset.batched(batch_size), num_workers=0, batch_size=None)\n",
    "\n",
    "# for image, target in islice(dataset, 0, 2):\n",
    "#     print(image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a3d38",
   "metadata": {},
   "source": [
    "### Original quality images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481c1a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59b1c536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patient_num = 1\n",
    "\n",
    "# for t, (x, y, z) in enumerate(loader_train):\n",
    "#     if t > 0:\n",
    "#         break\n",
    "#     tmp = x[patient_num, :, :, :].detach().numpy()\n",
    "# del x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "602133be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(5,8, figsize=(15, 6))\n",
    "# axs = axs.ravel()\n",
    "# for i in range(40):\n",
    "#     axs[i].imshow(tmp[i,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3730801c",
   "metadata": {},
   "source": [
    "### Downsampled quality images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c8111a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(5,8, figsize=(15, 6))\n",
    "# axs = axs.ravel()\n",
    "# for i in range(40):\n",
    "#     axs[i].imshow(st.resize(tmp[i,:,:], (256,256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "188d0945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del tmp\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e496afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "#     dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)\n",
    "print(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c8cf5845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(loader, model, criterion, iteration, writer):\n",
    "    print('Checking accuracy on validation set')\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        targets_np, scores_np, val_losses, batch_sizes = [], [], [], []\n",
    "        \n",
    "        # Run validation on validation batches\n",
    "        for x, y, z in loader:\n",
    "            \n",
    "            x = transforms.Resize(size=(256, 256))(x)\n",
    "\n",
    "            # Add code to unsqueeze because we only have 1 channel (axis=1) of this 3d image\n",
    "            x = x.unsqueeze(axis=1)\n",
    "            \n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "            scores = model(x)\n",
    "            val_loss = criterion(scores, y)\n",
    "            val_losses.append(val_loss.item())\n",
    "            batch_sizes.append(x.shape[0])\n",
    "            \n",
    "            scores_np.extend(scores.cpu().numpy())\n",
    "            targets_np.extend(y.cpu().numpy())\n",
    "        \n",
    "    # Calculate metrics after running full validation set\n",
    "    scores_np, targets_np = np.array(scores_np), np.array(targets_np)\n",
    "\n",
    "    # Log Metrics\n",
    "    val_loss = np.average(val_losses, weights=batch_sizes)\n",
    "    log_metrics(scores_np, targets_np, val_loss, iteration, writer, curr_mode=\"validation\")\n",
    "\n",
    "    # Print results\n",
    "    print('Total iteration %d, validation loss = %.4f' % (iteration, val_loss))\n",
    "#         print(\"Loss: {:.4f}, Micro accuracy: {:.3f}, Micro precision: {:.3f}, Micro recall: {:.3f}, Micro F1: {:.3f}\"\n",
    "#               .format(np.mean(val_losses), avg_dict['accuracy'], avg_dict['precision'], avg_dict['recall'], avg_dict['fscore'])\n",
    "#              )\n",
    "\n",
    "    # Return validation loss\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e7cf31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, loader_train, loader_val, log_dir, epochs=5, device=device, val_every=1):\n",
    "    \"\"\"\n",
    "    Train a model.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    total_iter = 0\n",
    "    val_loss_dict = {}\n",
    "    train_loss_dict = {}\n",
    "    \n",
    "    # Writer for tensorboard\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        ep_train_losses = []\n",
    "        ep_val_losses = []\n",
    "\n",
    "        print(\"************EPOCH: {:2d} ***************\".format(e))\n",
    "        for t, (x, y, z) in enumerate(loader_train):\n",
    "            t += 1 # To start from iteration 1 (1-indexing)\n",
    "            \n",
    "            model.train()  # put model to training mode\n",
    "            \n",
    "            \n",
    "            # Have to add resize to make dataset more manageable..\n",
    "            x = transforms.Resize(size=(256, 256))(x)\n",
    "            # Add code to unsqueeze because we only have 1 channel (axis=1) of this 3d image\n",
    "            x = x.unsqueeze(axis=1)\n",
    "            \n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = criterion(scores, y)\n",
    "            curr_batchloss = loss.item()\n",
    "            ep_train_losses.append(curr_batchloss)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Log metrics\n",
    "            scores_np = scores.detach().cpu().numpy()\n",
    "            targets_np = y.detach().cpu().numpy()\n",
    "            log_metrics(scores_np, targets_np, curr_batchloss, total_iter+t, writer, curr_mode='train')\n",
    "            \n",
    "            # Run validation step every val_every iteration\n",
    "            if t % val_every == 0:\n",
    "                print('Current epoch %d, epoch iteration %d, train loss = %.4f' % (e, t, curr_batchloss))\n",
    "                val_loss = validate_model(loader_val, model, criterion, total_iter+t, writer)\n",
    "                ep_val_losses.append(val_loss)\n",
    "                \n",
    "                # Also save checkpoint of model \n",
    "                ckpt_path = os.path.join(log_dir, 'Checkpoints', 'ep_%d_iter_%d_ckpt.pt' % (e, t))\n",
    "                torch.save({\n",
    "                    'epoch': e,\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'loss': loss,\n",
    "                }, ckpt_path)\n",
    "                \n",
    "                print()\n",
    "                writer.flush()\n",
    "                \n",
    "        train_loss_dict['epoch_'+str(e)] = ep_train_losses\n",
    "        val_loss_dict['epoch_'+str(e)] = ep_val_losses\n",
    "        total_iter += t+1\n",
    "        \n",
    "    # Close the summarywriter for tensorboard\n",
    "    writer.close()\n",
    "    \n",
    "    # Return train and validation loss\n",
    "    return (train_loss_dict, val_loss_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "694064f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make log directory and checkpoint directory\n",
    "dir_nm = datetime.now(tz=pytz.utc).astimezone(timezone('US/Pacific')).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "dir_nm = \"first_full_c2fc2\"\n",
    "log_dir = os.path.join('../runs', dir_nm)\n",
    "os.mkdir(log_dir)\n",
    "os.mkdir(os.path.join(log_dir, 'Checkpoints'))\n",
    "\n",
    "\n",
    "# Model, optimizer, criterion\n",
    "model = baseline_3DCNN(in_num_ch=1)\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48334108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18950274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train iterations per epoch: 58\n"
     ]
    }
   ],
   "source": [
    "shard_size = 32\n",
    "print(\"Number of train iterations per epoch:\", len(train_urls) * shard_size//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "98f0ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp  = (\n",
    "#     wds\n",
    "#     .WebDataset(sorted(train_urls)[:2])\n",
    "# )\n",
    "# for i, sample in enumerate(tmp):\n",
    "#     print(\"IIIIIIIIII:\", i)\n",
    "#     for key, value in sample.items():\n",
    "#         print(key, repr(value)[:50])\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9eaeb382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************EPOCH:  1 ***************\n",
      "torch.Size([16, 8, 5, 32, 32])\n",
      "torch.Size([16, 40960])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "000031.labels.pyd: duplicate file name in tar file labels.pyd dict_keys(['__key__', 'labels.pyd', 'studynames.pyd', 'volumes.pyd'])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-682b2dcc5f73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_loss_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-7d10fe175564>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, loader_train, loader_val, log_dir, epochs, device, val_every)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"************EPOCH: {:2d} ***************\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# To start from iteration 1 (1-indexing)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/webdataset/iterators.py\u001b[0m in \u001b[0;36mbatched\u001b[0;34m(data, batchsize, collation_fn, partial)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mbatchsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mcollation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/webdataset/iterators.py\u001b[0m in \u001b[0;36mto_tuple\u001b[0;34m(data, handler, *args)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgetfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_is_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/webdataset/iterators.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(data, f, handler)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreraise_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/webdataset/tariterators.py\u001b[0m in \u001b[0;36mgroup_by_keys\u001b[0;34m(data, keys, lcase, suffixes, handler)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcurrent_sample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             raise ValueError(\n\u001b[0;32m--> 171\u001b[0;31m                 \u001b[0;34mf\"{fname}: duplicate file name in tar file {suffix} {current_sample.keys()}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             )\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msuffixes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msuffix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuffixes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 000031.labels.pyd: duplicate file name in tar file labels.pyd dict_keys(['__key__', 'labels.pyd', 'studynames.pyd', 'volumes.pyd'])"
     ]
    }
   ],
   "source": [
    "train_loss_dict, val_loss_dict = train(model, optimizer, criterion, loader_train, loader_val, log_dir, epochs=10, device=device, val_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b0aa7b",
   "metadata": {},
   "source": [
    "### Loading a checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f71c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ckpt_path = os.path.join(log_dir, 'Checkpoints', 'ep_0_iter_3_ckpt.pt')\n",
    "# ckpt = torch.load(ckpt_path)\n",
    "\n",
    "# ckpt_model = baseline_3DCNN(in_num_ch=1)\n",
    "# ckpt_model.load_state_dict(ckpt['model_state_dict'])\n",
    "# ckpt_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abae3337",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m68"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
