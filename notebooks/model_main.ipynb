{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b86be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code credits: Adapted bits and pieces from https://github.com/webdataset/webdataset/blob/master/docs/gettingstarted.ipynb\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "from itertools import islice\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pytz import timezone\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import webdataset as wds\n",
    "\n",
    "from model.baseline_3d_cnn import *\n",
    "from utils.model_utils import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad94f525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../data'\n",
    "shards_dir = os.path.join(data_dir, 'shards_small')\n",
    "\n",
    "# Opening JSON file\n",
    "with open('../parameters.json') as json_file:\n",
    "    parameters = json.load(json_file)\n",
    "\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e46dbe3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train shards: 2\n",
      "Number of validation shards: 1\n",
      "Number of test shards: 18\n"
     ]
    }
   ],
   "source": [
    "urls = [os.path.join(shards_dir, it) for it in os.listdir(shards_dir) if it.endswith('.tar')]\n",
    "# All the data\n",
    "#train_urls = urls[:round(len(urls)*0.6)]\n",
    "#val_urls = urls[round(len(urls)*0.6):round(len(urls)*0.8)]\n",
    "#test_urls = urls[round(len(urls)*0.8):]\n",
    "\n",
    "# Smaller data just to run model once\n",
    "train_urls = urls[:2]\n",
    "val_urls = urls[2:3]\n",
    "test_urls = urls[3:]\n",
    "\n",
    "print(\"Number of train shards:\", len(train_urls))\n",
    "print(\"Number of validation shards:\", len(val_urls))\n",
    "print(\"Number of test shards:\", len(test_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4d2440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset objects\n",
    "train_dataset = (\n",
    "    wds\n",
    "    .WebDataset(train_urls)\n",
    "    .shuffle(parameters['batch_size'])\n",
    "    .decode('torch')\n",
    "    .to_tuple('volumes.pyd', 'labels.pyd', 'studynames.pyd')\n",
    ")\n",
    "loader_train = torch.utils.data.DataLoader(train_dataset.batched(1), num_workers=2, batch_size=None) #setting batch_size = None disables batching\n",
    "# parameters['batch_size']\n",
    "val_dataset = (\n",
    "    wds\n",
    "    .WebDataset(val_urls)\n",
    "    .shuffle(parameters['batch_size'])\n",
    "    .decode('torch')\n",
    "    .to_tuple('volumes.pyd', 'labels.pyd', 'studynames.pyd')\n",
    ")\n",
    "loader_val = torch.utils.data.DataLoader(val_dataset.batched(1), num_workers=2, batch_size=None)\n",
    "\n",
    "test_dataset = (\n",
    "    wds\n",
    "    .WebDataset(test_urls)\n",
    "    .shuffle(parameters['batch_size'])\n",
    "    .decode('torch')\n",
    "    .to_tuple('volumes.pyd', 'labels.pyd', 'studynames.pyd')\n",
    ")\n",
    "loader_test = torch.utils.data.DataLoader(test_dataset.batched(1), num_workers=2, batch_size=None)\n",
    "\n",
    "# for image, target in islice(dataset, 0, 2):\n",
    "#     print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd29e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "dtype = torch.float\n",
    "\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bd52ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(loader, model, criterion, iteration, writer):\n",
    "    print('Checking accuracy on validation set')\n",
    "\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()  # set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        targets_np, scores_np, val_losses, batch_sizes = [], [], [], []\n",
    "        \n",
    "        # Run validation on validation batches\n",
    "        for x, y, z in loader:\n",
    "            # Temporarily add code to unsqueeze #TEMP\n",
    "            x = x.unsqueeze(axis=1)\n",
    "            \n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            x = x.type(torch.cuda.FloatTensor)\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "            scores = model(x)\n",
    "            val_loss = criterion(scores, y)\n",
    "            val_losses.append(val_loss.item())\n",
    "            batch_sizes.append(x.shape[0])\n",
    "            \n",
    "            scores_np.extend(scores.cpu().numpy())\n",
    "            targets_np.extend(y.cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics after running full validation set\n",
    "        scores_np, targets_np = np.array(scores_np), np.array(targets_np)\n",
    "        \n",
    "        # Log Metrics\n",
    "        val_loss = np.average(val_losses, weights=batch_sizes)\n",
    "        log_metrics(scores_np, targets_np, val_loss, iteration, writer, curr_mode=\"validation\")\n",
    "        \n",
    "        # Print results\n",
    "        print('Total iteration %d, validation loss = %.4f' % (iteration, val_loss))\n",
    "#         print(\"Loss: {:.4f}, Micro accuracy: {:.3f}, Micro precision: {:.3f}, Micro recall: {:.3f}, Micro F1: {:.3f}\"\n",
    "#               .format(np.mean(val_losses), avg_dict['accuracy'], avg_dict['precision'], avg_dict['recall'], avg_dict['fscore'])\n",
    "#              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb39e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, loader_train, loader_val, writer, epochs=5, device=device, val_every=1):\n",
    "    \"\"\"\n",
    "    Train a model.\n",
    "    \n",
    "    Inputs:\n",
    "    - model: A PyTorch Module giving the model to train.\n",
    "    - optimizer: An Optimizer object we will use to train the model\n",
    "    - epochs: (Optional) A Python integer giving the number of epochs to train for\n",
    "    \n",
    "    Returns: Nothing, but prints model accuracies during training.\n",
    "    \"\"\"\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    batch_losses = []\n",
    "    total_iter = 0\n",
    "    for e in range(epochs):\n",
    "        print(\"************EPOCH: {:2d} ***************\".format(e))\n",
    "        for t, (x, y, z) in enumerate(loader_train):\n",
    "            model.train()  # put model to training mode\n",
    "            \n",
    "            # Temporarily add code to unsqueeze #TEMP\n",
    "            x = x.unsqueeze(axis=1)\n",
    "            \n",
    "            x = x.to(device=device)  # move to device, e.g. GPU\n",
    "            x = x.type(torch.cuda.FloatTensor)\n",
    "\n",
    "            y = y.to(device=device, dtype=dtype)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = criterion(scores, y)\n",
    "            curr_batchloss = loss.item()\n",
    "            batch_losses.append(curr_batchloss)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Log metrics\n",
    "            scores_np = scores.detach().cpu().numpy()\n",
    "            targets_np = y.detach().cpu().numpy()\n",
    "            log_metrics(scores_np, targets_np, curr_batchloss, total_iter+t, writer, curr_mode='train')\n",
    "            \n",
    "            # Run validation step\n",
    "            if t % val_every == 0:\n",
    "                print('Current epoch %d, epoch iteration %d, train loss = %.4f' % (e, t, curr_batchloss))\n",
    "                validate_model(loader_val, model, criterion, total_iter+t, writer)\n",
    "                print()\n",
    "                writer.flush()\n",
    "        total_iter += t+1\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23940086",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_3DCNN(in_num_ch=1)\n",
    "\n",
    "currtime = datetime.now(tz=pytz.utc).astimezone(timezone('US/Pacific')).strftime('%Y-%m-%d_%H-%M-%S')\n",
    "writer = SummaryWriter(log_dir=os.path.join('../runs', currtime))\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d937dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09fc159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************EPOCH:  0 ***************\n",
      "torch.Size([1, 16, 5, 64, 64])\n",
      "Current epoch 0, epoch iteration 0, train loss = 0.6811\n",
      "Checking accuracy on validation set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.7/site-packages/webdataset/dataset.py:139: UserWarning: num_workers 2 > num_shards 1\n",
      "  warnings.warn(f\"num_workers {num_workers} > num_shards {len(urls)}\")\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Caught MemoryError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 34, in fetch\n    data = next(self.dataset_iter)\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/iterators.py\", line 359, in batched\n    for sample in data:\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/iterators.py\", line 282, in to_tuple\n    for sample in data:\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/iterators.py\", line 226, in map\n    for sample in data:\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/iterators.py\", line 190, in shuffle\n    buf.append(next(data))  # skipcq: PYL-R1708\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/tariterators.py\", line 153, in group_by_keys\n    for fname, value in data:\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/tariterators.py\", line 139, in tar_file_expander\n    if handler(exn):\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/utils.py\", line 12, in reraise_exception\n    raise exn\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/tariterators.py\", line 135, in tar_file_expander\n    for sample in tar_file_iterator(source[\"stream\"]):\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/tariterators.py\", line 119, in tar_file_iterator\n    if handler(exn):\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/utils.py\", line 12, in reraise_exception\n    raise exn\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/tariterators.py\", line 116, in tar_file_iterator\n    data = stream.extractfile(tarinfo).read()\n  File \"/opt/conda/lib/python3.7/tarfile.py\", line 695, in read\n    b = self.fileobj.read(length)\n  File \"/opt/conda/lib/python3.7/tarfile.py\", line 537, in read\n    buf = self._read(size)\n  File \"/opt/conda/lib/python3.7/tarfile.py\", line 545, in _read\n    return self.__read(size)\n  File \"/opt/conda/lib/python3.7/tarfile.py\", line 577, in __read\n    return t[:size]\nMemoryError\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9cf7f2f4527d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b8b9c983f52e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, loader_train, loader_val, writer, epochs, device, val_every)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mval_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Current epoch %d, epoch iteration %d, train loss = %.4f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_batchloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m                 \u001b[0mvalidate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_iter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-318135007a37>\u001b[0m in \u001b[0;36mvalidate_model\u001b[0;34m(loader, model, criterion, iteration, writer)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Run validation on validation batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;31m# Temporarily add code to unsqueeze #TEMP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1223\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Caught MemoryError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 202, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 34, in fetch\n    data = next(self.dataset_iter)\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/iterators.py\", line 359, in batched\n    for sample in data:\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/iterators.py\", line 282, in to_tuple\n    for sample in data:\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/iterators.py\", line 226, in map\n    for sample in data:\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/iterators.py\", line 190, in shuffle\n    buf.append(next(data))  # skipcq: PYL-R1708\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/tariterators.py\", line 153, in group_by_keys\n    for fname, value in data:\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/tariterators.py\", line 139, in tar_file_expander\n    if handler(exn):\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/utils.py\", line 12, in reraise_exception\n    raise exn\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/tariterators.py\", line 135, in tar_file_expander\n    for sample in tar_file_iterator(source[\"stream\"]):\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/tariterators.py\", line 119, in tar_file_iterator\n    if handler(exn):\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/utils.py\", line 12, in reraise_exception\n    raise exn\n  File \"/opt/conda/lib/python3.7/site-packages/webdataset/tariterators.py\", line 116, in tar_file_iterator\n    data = stream.extractfile(tarinfo).read()\n  File \"/opt/conda/lib/python3.7/tarfile.py\", line 695, in read\n    b = self.fileobj.read(length)\n  File \"/opt/conda/lib/python3.7/tarfile.py\", line 537, in read\n    buf = self._read(size)\n  File \"/opt/conda/lib/python3.7/tarfile.py\", line 545, in _read\n    return self.__read(size)\n  File \"/opt/conda/lib/python3.7/tarfile.py\", line 577, in __read\n    return t[:size]\nMemoryError\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, loader_train, loader_val, writer, epochs=3, device=device, val_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15af752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-8.m68",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-8:m68"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
